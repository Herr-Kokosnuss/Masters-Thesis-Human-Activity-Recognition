{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries import \n",
    "import cv2\n",
    "import datetime as dt\n",
    "import math\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import rarfile\n",
    "import seaborn as sns\n",
    "import ssl\n",
    "import time\n",
    "import urllib\n",
    "import urllib.request\n",
    "from collections import defaultdict, deque\n",
    "from IPython.display import Video\n",
    "from keras.layers import BatchNormalization, Conv2D, Dense, Dropout, Flatten, LSTM, MaxPooling2D, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from moviepy.editor import *\n",
    "from sklearn.metrics import auc, classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.ops import non_max_suppression\n",
    "from urllib.request import urlretrieve\n",
    "from yt_dlp import YoutubeDL\n",
    "import yt_dlp as youtube_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting constant seed\n",
    "seed_constant = 27\n",
    "\n",
    "# Set the seed for NumPy random number generator to 'seed_constant'.\n",
    "np.random.seed(seed_constant)\n",
    "\n",
    "# Set the seed for Python's built-in random number generator to 'seed_constant'.\n",
    "random.seed(seed_constant)\n",
    "\n",
    "# Set the seed for TensorFlow's random number generator to 'seed_constant'.\n",
    "tf.random.set_seed(seed_constant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Download\n",
    "\n",
    "# Define the URL of the UCF50 dataset.\n",
    "DATA_URL = 'https://www.crcv.ucf.edu/data/UCF50.rar'\n",
    "\n",
    "# Define the local directory path where the dataset will be downloaded.\n",
    "DATA_PATH = 'workspace'\n",
    "\n",
    "# Create the complete path to the directory where the UCF50 dataset will be stored after extraction.\n",
    "UCF50_DATA_PATH = os.path.join(DATA_PATH, 'UCF50')\n",
    "\n",
    "# Check if the directory specified by DATA_PATH already exists.\n",
    "if os.path.exists(DATA_PATH):\n",
    "\n",
    "    # If the directory exists, print a message indicating that the data is already available.\n",
    "    print('[INFO] Data already exists.')\n",
    "\n",
    "else:\n",
    "\n",
    "    # If the directory specified by DATA_PATH does not exist, execute the following block.\n",
    "\n",
    "    # Print a message indicating that the data is being downloaded.\n",
    "    print('[INFO] Downloading data in the data directory.')\n",
    "\n",
    "    # Create the DATA_PATH directory on the local file system.\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "    # Create a default SSL context with an unverified SSL certificate to allow downloading data.\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    # Download the UCF50 dataset from the specified DATA_URL and save it as 'UCF50.rar' in the DATA_PATH directory.\n",
    "    urlretrieve(url=DATA_URL, filename=os.path.join(DATA_PATH, 'UCF50.rar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Extraction \n",
    "\n",
    "# Check if the directory specified by UCF50_DATA_PATH already exists.\n",
    "if os.path.exists(UCF50_DATA_PATH):\n",
    "\n",
    "    # If the directory exists, print a message indicating that the data is already available, and the extraction process is skipped.\n",
    "    print('[INFO] UCF50 Data already exists, skipping extraction process.')\n",
    "\n",
    "else:\n",
    "\n",
    "    # If the directory specified by UCF50_DATA_PATH does not exist, execute the following block.\n",
    "\n",
    "    # Print a message indicating that the data is being extracted to the UCF50_DATA_PATH directory.\n",
    "    print(f'[INFO] Extracting data: \"{UCF50_DATA_PATH}\"')\n",
    "\n",
    "    # Create a RarFile object 'r' to open and read the 'UCF50.rar' archive file.\n",
    "    r = rarfile.RarFile('/workspace/Workspace/UCF50.rar')\n",
    "\n",
    "    # Extract all files and directories from the 'UCF50.rar' archive to the DATA_PATH directory.\n",
    "    r.extractall(DATA_PATH)\n",
    "\n",
    "    # Close the RarFile object to release resources.\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preview\n",
    "# Create a Matplotlib figure with a specified size of 20x20 inches.\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Get the list of all class names in the UCF50 directory.\n",
    "all_classes_names = os.listdir('workspace/UCF50')\n",
    "\n",
    "# Randomly select 10 class names from the list of all classes.\n",
    "random_range = random.sample(range(len(all_classes_names)), 10)\n",
    "\n",
    "# Iterate through the randomly selected class indices and their corresponding counter.\n",
    "for counter, random_index in enumerate(random_range, 1):\n",
    "\n",
    "    # Get the selected class name using the random index.\n",
    "    selected_class_Name = all_classes_names[random_index]\n",
    "\n",
    "    # Get the list of video file names inside the selected class directory.\n",
    "    #video_files_names_list = os.listdir(f'UCF50/{selected_class_Name}')\n",
    "    base_path = 'workplace/UCF50/'\n",
    "    class_folder_path = os.path.join(base_path, selected_class_Name)\n",
    "    video_files_names_list = os.listdir(class_folder_path)\n",
    "\n",
    "    # Randomly select a video file name from the list of video files.\n",
    "    selected_video_file_name = random.choice(video_files_names_list)\n",
    "\n",
    "    # Initialize a VideoCapture object to read from the randomly selected video file.\n",
    "    video_reader = cv2.VideoCapture(f'UCF50/{selected_class_Name}/{selected_video_file_name}')\n",
    "    \n",
    "    # Read the first frame from the video file.\n",
    "    _, bgr_frame = video_reader.read()\n",
    "\n",
    "    # Release the VideoCapture object to free up resources.\n",
    "    video_reader.release()\n",
    "\n",
    "    # Convert the frame from BGR to RGB format for display in Matplotlib.\n",
    "    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Add the selected class name as text on the video frame.\n",
    "    cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (125, 246, 55), 2)\n",
    "    \n",
    "    # Create a subplot in the figure and display the RGB frame in the subplot.\n",
    "    plt.subplot(5, 4, counter)\n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Video Classification Constants and Variables\n",
    "# Set the height and width of the video frames to 128x128 pixels.\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 128, 128\n",
    "\n",
    "# Set the sequence length, which represents the number of frames per video sequence.\n",
    "SEQUENCE_LENGTH = 15\n",
    "\n",
    "# Define the list of classes that will be used for training the model.\n",
    "SELECTED_CLASSES = ['Kayaking', 'Basketball', 'JumpRope']\n",
    "#SELECTED_CLASSES = ['Kayaking', 'Basketball', 'JumpRope', 'Diving', 'HorseRace', 'PullUps','MilitaryParade']\n",
    "#SELECTED_CLASSES = ['BenchPress', 'PullUps', 'PushUps'] # Special classes list\n",
    "\n",
    "\n",
    "# Set the path to the dataset directory, which contains the UCF50 dataset.\n",
    "DATASET_DIR = 'workspace/UCF50'    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose and Drawing utilities.\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the MediaPipe Pose model.\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "\n",
    "def preprocess_landmarks(landmarks):\n",
    "    # Normalize the landmarks to be relative to the hip center\n",
    "    hip_center = np.mean([landmarks[23], landmarks[24]], axis=0)\n",
    "    normalized_landmarks = landmarks - hip_center\n",
    "    \n",
    "    # Flatten the landmarks array\n",
    "    flattened = normalized_landmarks.flatten()\n",
    "    \n",
    "    # Normalize the flattened array\n",
    "    return (flattened - np.mean(flattened)) / np.std(flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save and Load Pre-Processed Data\n",
    "def save_extracted_data(frames, labels, file_name):\n",
    "    if not os.path.exists('pre-processed'):\n",
    "        os.makedirs('pre-processed')\n",
    "    np.savez_compressed(os.path.join('pre-processed', file_name), frames=frames, labels=labels)\n",
    "\n",
    "def load_preprocessed_data(file_name):\n",
    "    data = np.load(os.path.join('pre-processed', file_name))\n",
    "    return data['frames'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess Landmarks\n",
    "def preprocess_landmarks(landmarks):\n",
    "    # Normalize the landmarks to be relative to the hip center\n",
    "    hip_center = np.mean([landmarks[23], landmarks[24]], axis=0)\n",
    "    normalized_landmarks = landmarks - hip_center\n",
    "    \n",
    "    # Flatten the landmarks array\n",
    "    flattened = normalized_landmarks.flatten()\n",
    "    \n",
    "    # Normalize the flattened array\n",
    "    return (flattened - np.mean(flattened)) / np.std(flattened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract frames from a video\n",
    "def extract_frames(video_path):\n",
    "    # Initialize empty lists to store video frames and poses.\n",
    "    frames_list = []\n",
    "    pose_list = []\n",
    "    \n",
    "    # Open the video file for reading using OpenCV's VideoCapture object.\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get the total number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Calculate the interval between frames to be added to the frames list.\n",
    "    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    # Loop through the specified number of frames (SEQUENCE_LENGTH).\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "        \n",
    "        # Set the position of the video to the appropriate frame.\n",
    "        # This allows us to extract non-consecutive frames from the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "        \n",
    "        # Read the frame from the current position in the video.\n",
    "        # 'success' will be True if the frame is read successfully.\n",
    "        # 'frame' contains the frame data (an image in the form of a NumPy array).\n",
    "        success, frame = video_reader.read()\n",
    "        \n",
    "        # If the frame cannot be read successfully (e.g., end of video reached), exit the loop.\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Resize the frame to a fixed height and width (IMAGE_HEIGHT, IMAGE_WIDTH).\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        \n",
    "        # Normalize the pixel values of the resized frame to the range [0, 1].\n",
    "        # Divide by 255 to convert pixel values from [0, 255] to [0, 1].\n",
    "        normalized_frame = resized_frame / 255.0\n",
    "\n",
    "        # Convert the frame from BGR to RGB color format.\n",
    "        frame_rgb = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect the pose landmarks.\n",
    "        results = pose.process(frame_rgb)\n",
    "        \n",
    "        # Append the normalized frame to the frames_list.\n",
    "        frames_list.append(normalized_frame)\n",
    "\n",
    "        # Extract the pose landmarks from the frame.\n",
    "        if results.pose_landmarks:\n",
    "            \n",
    "            # Get the pose landmarks from the results.\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Convert the landmarks to a NumPy array.\n",
    "            pose_landmarks = np.array([(lm.x, lm.y, lm.z) for lm in landmarks])\n",
    "\n",
    "            # Preprocess the pose landmarks to normalize the coordinates.\n",
    "            preprocessed_landmarks = preprocess_landmarks(pose_landmarks)\n",
    "            \n",
    "            # Append the preprocessed landmarks to the pose_list.\n",
    "            pose_list.append(preprocessed_landmarks)\n",
    "\n",
    "            # Drawing the pose landmarks.\n",
    "            if frame_counter == 0:\n",
    "                \n",
    "                # Draw the pose landmarks on the first frame only.\n",
    "                mp_drawing.draw_landmarks(resized_frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "                # Save the frame with the pose landmarks drawn as an image file.\n",
    "                cv2.imwrite('sample_frame_with_pose.jpg', resized_frame)            \n",
    "        else:\n",
    "            # If no pose landmarks are detected, append a list of zeros to the pose_list.\n",
    "            pose_list.append(np.zeros(99))  # 33 landmarks * 3 coordinates\n",
    "\n",
    "    # Release the VideoCapture object to free up resources.\n",
    "    video_reader.release()\n",
    "    \n",
    "    # Return the list of normalized frames and poses extracted from the video.\n",
    "    return frames_list, pose_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save and Load Dataset\n",
    "def save_dataset(frame_features, pose_features, labels, filename='dataset-3-special.pkl'):\n",
    "    \"\"\"Save the extracted features and labels to a file.\"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump((frame_features, pose_features, labels), f)\n",
    "    print(f\"Dataset saved to {filename}\")\n",
    "\n",
    "def load_dataset(filename='dataset-3-special.pkl'):\n",
    "    \"\"\"Load the extracted features and labels from a file.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        frame_features, pose_features, labels = pickle.load(f)\n",
    "    print(f\"Dataset loaded from {filename}\")\n",
    "    return frame_features, pose_features, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Video Data\n",
    "def extract_video_data(save=True, filename='dataset-3-special.pkl'):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Loading existing dataset from {filename}\")\n",
    "        return load_dataset(filename)\n",
    "    extracted_frames = []\n",
    "    extracted_poses = []\n",
    "    extracted_labels = []\n",
    "\n",
    "\n",
    "    \n",
    "    for class_index, class_name in enumerate(SELECTED_CLASSES):\n",
    "        print(f'Data Extracting of Class: {class_name}')\n",
    "\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "        for file_name in files_list:\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    "            frames, poses = extract_frames(video_file_path)\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "                extracted_frames.append(frames)\n",
    "                extracted_poses.append(poses)\n",
    "                extracted_labels.append(class_index)\n",
    "\n",
    "    #return np.array(extracted_frames), np.array(extracted_poses), np.array(extracted_labels)\n",
    "    frame_features = np.array(extracted_frames)\n",
    "    pose_features = np.array(extracted_poses)\n",
    "    labels = np.array(extracted_labels)\n",
    "\n",
    "    if save:\n",
    "        save_dataset(frame_features, pose_features, labels, filename)\n",
    "\n",
    "    return frame_features, pose_features, labels\n",
    "\n",
    "# Extract features and labels\n",
    "#frame_features, pose_features, labels = extract_video_data()\n",
    "frame_features, pose_features, labels = extract_video_data(save=True)\n",
    "\n",
    "# One-hot encode labels\n",
    "one_hot_encoded_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train_frames, X_test_frames, X_train_poses, X_test_poses, y_train, y_test = train_test_split(\n",
    "    frame_features, pose_features, one_hot_encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training features and labels after the split.\n",
    "X_train_frames.shape, X_train_poses.shape, y_train.shape\n",
    "\n",
    "## Print the shapes of the test features and labels after the split.\n",
    "X_test_frames.shape, X_test_poses.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optuna Optimization\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to be optimized\n",
    "    num_conv_layers = trial.suggest_int('num_conv_layers', 1, 4)\n",
    "    #num_filters = trial.suggest_categorical('num_filters', (16, 256))\n",
    "    num_filters = trial.suggest_int('num_filters', 16, 256, step=16)\n",
    "    kernel_size = trial.suggest_categorical('kernel_size', [(3, 3), (5, 5)])\n",
    "    lstm_units = trial.suggest_int('lstm_units', 16, 256, step = 16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    epochs = trial.suggest_int('epochs', 5, 50)\n",
    "    batch_size = trial.suggest_int('batch_size', 8, 32, step = 16)\n",
    "\n",
    "    # Build the combined model\n",
    "    # Image input branch\n",
    "    image_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "    x1 = image_input\n",
    "    for _ in range(num_conv_layers):\n",
    "        x1 = TimeDistributed(Conv2D(num_filters, kernel_size, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))(x1)\n",
    "        x1 = TimeDistributed(MaxPooling2D((2, 2)))(x1)\n",
    "        x1 = TimeDistributed(Dropout(dropout_rate))(x1)\n",
    "    x1 = TimeDistributed(Flatten())(x1)\n",
    "    x1 = LSTM(lstm_units, kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "\n",
    "    # Pose input branch\n",
    "    pose_input = Input(shape=(SEQUENCE_LENGTH, 99))\n",
    "    x2 = LSTM(lstm_units, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))(pose_input)\n",
    "    x2 = LSTM(16)(x2)\n",
    "    \n",
    "    # Combine the two branches\n",
    "    combined = concatenate([x1, x2])\n",
    "    x = Dense(64, activation='relu')(combined)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output = Dense(len(SELECTED_CLASSES), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[image_input, pose_input], outputs=output)\n",
    "\n",
    "    # Compile the model with weight decay\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping and learning rate reduction callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([X_train_frames, X_train_poses], y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2,\n",
    "                        callbacks=[TFKerasPruningCallback(trial, 'val_accuracy'), early_stopping, reduce_lr], verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate([X_test_frames, X_test_poses], y_test, verbose=0)\n",
    "    return score[1]  # Return validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pruner with min_resource\n",
    "pruner = SuccessiveHalvingPruner(min_resource=10, reduction_factor=3)\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters: ', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create LRCN Model\n",
    "def create_best_LRCN_model(best_params):\n",
    "    best_params = best_params\n",
    "    \n",
    "    # Image input branch\n",
    "    image_input = Input(shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3), name='image_input')\n",
    "    x1 = image_input\n",
    "    for _ in range(best_params['num_conv_layers']):\n",
    "        x1 = TimeDistributed(Conv2D(best_params['num_filters'], best_params['kernel_size'], padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))(x1)\n",
    "        x1 = TimeDistributed(MaxPooling2D((2, 2)))(x1)\n",
    "        x1 = TimeDistributed(Dropout(best_params['dropout_rate']))(x1)\n",
    "    x1 = TimeDistributed(Flatten())(x1)\n",
    "    x1 = LSTM(best_params['lstm_units'], kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "\n",
    "    # Pose input branch\n",
    "    pose_input = Input(shape=(SEQUENCE_LENGTH, 99), name='pose_input')  # POSE_FEATURES should be defined based on your pose data\n",
    "    x2 = LSTM(best_params['lstm_units'], return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))(pose_input)\n",
    "    x2 = LSTM(16)(x2)\n",
    "\n",
    "    # Combine the two branches\n",
    "    combined = concatenate([x1, x2])\n",
    "    x = Dense(64, activation='relu')(combined)\n",
    "    x = Dropout(best_params['dropout_rate'])(x)\n",
    "    output = Dense(len(SELECTED_CLASSES), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[image_input, pose_input], outputs=output)\n",
    "\n",
    "    # Compile the model with weight decay\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "model = create_best_LRCN_model(best_params)\n",
    "#model = create_combined_model()\n",
    "# Display the success message.\n",
    "print(\"Model Created Successfully!\")\n",
    "\n",
    "# Count the number of layers\n",
    "num_layers = len(model.layers)\n",
    "print(f'The model has {num_layers} layers.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the model\n",
    "learning_rate = best_params['learning_rate']\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model with the custom optimizer\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_frames, X_train_poses], y_train,\n",
    "    validation_data=([X_test_frames, X_test_poses], y_test),\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "model.save('best_model_fusion-3-special.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model.\n",
    "# X_train_frames, X_test_frames, X_train_poses, X_test_poses, y_train, y_test\n",
    "model_evaluation_history = model.evaluate([X_test_frames, X_test_poses], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Use the existing test data\n",
    "for frames, poses, labels in tqdm(zip(X_test_frames, X_test_poses, y_test), desc=\"Testing\", total=len(X_test_frames)):\n",
    "    outputs = model.predict([np.expand_dims(frames, axis=0), np.expand_dims(poses, axis=0)])\n",
    "    predicted = np.argmax(outputs, axis=1)\n",
    "    all_predictions.extend(predicted)\n",
    "    all_labels.extend(np.argmax(labels.reshape(1, -1), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Training Curves\n",
    "def plot_training_curves(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "# Plot training curves\n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (Percentages)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('confusion_matrix_percent.png')\n",
    "    plt.close()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(all_labels, all_predictions, SELECTED_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Classification Report\n",
    "def plot_classification_report(y_true, y_pred, classes):\n",
    "    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df = df.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
    "    df = df.drop('support', axis=1)  # Remove the support column\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=df.values.round(2),\n",
    "                     rowLabels=df.index,\n",
    "                     colLabels=df.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "\n",
    "    for i, key in enumerate(df.columns):\n",
    "        cell = table[0, i]\n",
    "        cell.set_text_props(weight='bold', color='white')\n",
    "        cell.set_facecolor('#4C72B0')\n",
    "\n",
    "    plt.title('Classification Report', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('classification_report.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Plot classification report\n",
    "plot_classification_report(all_labels, all_predictions, SELECTED_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC Curve\n",
    "def plot_roc_curve(all_labels, all_predictions, SELECTED_CLASSES):\n",
    "    y_true = label_binarize(all_labels, classes=range(len(SELECTED_CLASSES)))\n",
    "    y_pred = label_binarize(all_predictions, classes=range(len(SELECTED_CLASSES)))\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(SELECTED_CLASSES)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = plt.cm.get_cmap('Set1')(np.linspace(0, 1, len(SELECTED_CLASSES)))\n",
    "    for i, color in zip(range(len(SELECTED_CLASSES)), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of {SELECTED_CLASSES[i]} (area = {roc_auc[i]:0.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\", fontsize='small')\n",
    "    plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(all_labels, all_predictions, SELECTED_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_analyze_video(video_file_path, output_video_path, output_graph_path, model, SEQUENCE_LENGTH, SELECTED_CLASSES):\n",
    "    # Suppress TensorFlow logging\n",
    "    tf.get_logger().setLevel('ERROR')\n",
    "    \n",
    "    # Video setup\n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    "    fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                                   fps, (original_video_width, original_video_height))\n",
    "\n",
    "    # Initialize variables for prediction and analysis\n",
    "    frames_queue = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    pose_queue = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    frame_predictions = []\n",
    "    class_probabilities = defaultdict(list)\n",
    "    predicted_class_name = ''\n",
    "\n",
    "    # Initialize MediaPipe Pose\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose = mp_pose.Pose()\n",
    "\n",
    "    # Process video frames\n",
    "    with tqdm(total=frame_count, desc=\"Processing video\") as pbar:\n",
    "        while video_reader.isOpened():\n",
    "            ok, frame = video_reader.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            # Preprocess the frame\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            resized_frame = cv2.resize(frame_rgb, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "            normalized_frame = resized_frame / 255.0\n",
    "\n",
    "            # Process the frame for pose detection\n",
    "            results = pose.process(frame_rgb)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                # Draw pose landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    results.pose_landmarks, \n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "                # Extract and preprocess landmarks for prediction\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                pose_landmarks = np.array([(lm.x, lm.y, lm.z) for lm in landmarks])\n",
    "                preprocessed_landmarks = preprocess_landmarks(pose_landmarks)\n",
    "            else:\n",
    "                preprocessed_landmarks = np.zeros(99)  # 33 landmarks * 3 coordinates\n",
    "\n",
    "            frames_queue.append(normalized_frame)\n",
    "            pose_queue.append(preprocessed_landmarks)\n",
    "\n",
    "            if len(frames_queue) == SEQUENCE_LENGTH:\n",
    "                # Prepare input for the model\n",
    "                input_frames = np.expand_dims(np.array(frames_queue), axis=0)\n",
    "                input_poses = np.expand_dims(np.array(pose_queue), axis=0)\n",
    "\n",
    "                # Get predictions\n",
    "                with tf.device('/GPU:0'):\n",
    "                    outputs = model.predict([input_frames, input_poses], verbose=0)\n",
    "                probabilities = outputs[0]\n",
    "                predicted_class_index = np.argmax(probabilities)\n",
    "                predicted_class_name = SELECTED_CLASSES[predicted_class_index]\n",
    "\n",
    "                # Store predictions and probabilities\n",
    "                frame_predictions.append(predicted_class_name)\n",
    "                for i, class_name in enumerate(SELECTED_CLASSES):\n",
    "                    class_probabilities[class_name].append(probabilities[i])\n",
    "            else:\n",
    "                frame_predictions.append(None)\n",
    "                for class_name in SELECTED_CLASSES:\n",
    "                    class_probabilities[class_name].append(0)\n",
    "\n",
    "            # Write predicted class name on top of the frame\n",
    "            cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Write the frame into the disk using the VideoWriter Object\n",
    "            video_writer.write(frame)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    video_reader.release()\n",
    "    video_writer.release()\n",
    "    pose.close()\n",
    "\n",
    "    # Pad the beginning of predictions and probabilities\n",
    "    pad_length = SEQUENCE_LENGTH - 1\n",
    "    frame_predictions = [None] * pad_length + frame_predictions[pad_length:]\n",
    "    for class_name in SELECTED_CLASSES:\n",
    "        class_probabilities[class_name] = [0] * pad_length + class_probabilities[class_name][pad_length:]\n",
    "\n",
    "    # Plot frame-by-frame results\n",
    "    plot_frame_by_frame_results(frame_predictions, class_probabilities, fps, output_graph_path)\n",
    "\n",
    "    return frame_predictions, class_probabilities, fps\n",
    "#Plot Frame by Frame Results\n",
    "def plot_frame_by_frame_results(frame_predictions, class_probabilities, fps, output_path):\n",
    "    frame_count = len(frame_predictions)\n",
    "    time_axis = np.arange(frame_count) / fps\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot class probabilities\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for class_name, probs in class_probabilities.items():\n",
    "        plt.plot(time_axis, probs, label=class_name)\n",
    "    plt.title(\"Class Probabilities Over Time\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot predicted classes\n",
    "    plt.subplot(2, 1, 2)\n",
    "    unique_classes = list(set(frame_predictions) - {None})\n",
    "    class_to_num = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "    numeric_predictions = [class_to_num[cls] if cls is not None else -1 for cls in frame_predictions]\n",
    "    plt.scatter(time_axis, numeric_predictions, marker='.')\n",
    "    plt.yticks(range(len(unique_classes)), unique_classes)\n",
    "    plt.title(\"Predicted Class Over Time\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Predicted Class\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process Test Video\n",
    "def process_test_video(model, SEQUENCE_LENGTH, SELECTED_CLASSES):\n",
    "    test_videos_directory = 'test_videos'\n",
    "    video_title = 'pu3'  # Replace with the actual video title\n",
    "\n",
    "    input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'\n",
    "    output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'\n",
    "    output_graph_path = f'{test_videos_directory}/{video_title}_frame_analysis.png'\n",
    "\n",
    "    frame_predictions, class_probabilities, fps = predict_and_analyze_video(\n",
    "        input_video_file_path, output_video_file_path, output_graph_path,\n",
    "        model, SEQUENCE_LENGTH, SELECTED_CLASSES\n",
    "    )\n",
    "\n",
    "    print(f\"Processed video saved to: {output_video_file_path}\")\n",
    "    print(f\"Frame-by-frame analysis graph saved to: {output_graph_path}\")\n",
    "\n",
    "# Call this function after training your model\n",
    "process_test_video(model, SEQUENCE_LENGTH, SELECTED_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRA:\n",
    "# Function to download videos from YouTube.\n",
    "def download_yt_videos(yt_url_list, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'outtmpl': os.path.join(save_dir, '%(title)s.%(ext)s'),\n",
    "        'format': 'bestvideo+bestaudio/best',\n",
    "        'merge_output_format': 'mp4'\n",
    "    }\n",
    "    \n",
    "    for url in yt_url_list:\n",
    "        try:\n",
    "            with YoutubeDL(ydl_opts) as ydl:\n",
    "                ydl.download([url])\n",
    "                print(f\"Downloaded: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "yt_url_list = [\n",
    "    'https://www.youtube.com/shorts/y7PBQ2fYbxY'\n",
    "    \n",
    "]\n",
    "save_dir = 'test_data'\n",
    "download_yt_videos(yt_url_list, save_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
