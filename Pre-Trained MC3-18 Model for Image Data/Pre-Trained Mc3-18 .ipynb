{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import cv2\n",
    "from IPython.display import Video\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "from moviepy.editor import *\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pytube import YouTube\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ssl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.video import mc3_18, MC3_18_Weights\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.ops import non_max_suppression\n",
    "from urllib.request import urlretrieve\n",
    "from yt_dlp import YoutubeDL\n",
    "import yt_dlp as youtube_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset download\n",
    "\n",
    "# Define the URL of the UCF50 dataset.\n",
    "DATA_URL = 'https://www.crcv.ucf.edu/data/UCF50.rar'\n",
    "\n",
    "# Define the local directory path where the dataset will be downloaded.\n",
    "DATA_PATH = 'workspace'\n",
    "\n",
    "# Create the complete path to the directory where the UCF50 dataset will be stored after extraction.\n",
    "UCF50_DATA_PATH = os.path.join(DATA_PATH, 'UCF50')\n",
    "\n",
    "# Check if the directory specified by DATA_PATH already exists.\n",
    "if os.path.exists(DATA_PATH):\n",
    "\n",
    "    # If the directory exists, print a message indicating that the data is already available.\n",
    "    print('[INFO] Data already exists.')\n",
    "\n",
    "else:\n",
    "\n",
    "    # If the directory specified by DATA_PATH does not exist, execute the following block.\n",
    "\n",
    "    # Print a message indicating that the data is being downloaded.\n",
    "    print('[INFO] Downloading data in the data directory.')\n",
    "\n",
    "    # Create the DATA_PATH directory on the local file system.\n",
    "    os.mkdir(DATA_PATH)\n",
    "\n",
    "    # Create a default SSL context with an unverified SSL certificate to allow downloading data.\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "    # Download the UCF50 dataset from the specified DATA_URL and save it as 'UCF50.rar' in the DATA_PATH directory.\n",
    "    urlretrieve(url=DATA_URL, filename=os.path.join(DATA_PATH, 'UCF50.rar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Extraction \n",
    "\n",
    "# Check if the directory specified by UCF50_DATA_PATH already exists.\n",
    "if os.path.exists(UCF50_DATA_PATH):\n",
    "\n",
    "    # If the directory exists, print a message indicating that the data is already available, and the extraction process is skipped.\n",
    "    print('[INFO] UCF50 Data already exists, skipping extraction process.')\n",
    "\n",
    "else:\n",
    "\n",
    "    # If the directory specified by UCF50_DATA_PATH does not exist, execute the following block.\n",
    "\n",
    "    # Print a message indicating that the data is being extracted to the UCF50_DATA_PATH directory.\n",
    "    print(f'[INFO] Extracting data: \"{UCF50_DATA_PATH}\"')\n",
    "\n",
    "    # Create a RarFile object 'r' to open and read the 'UCF50.rar' archive file.\n",
    "    r = rarfile.RarFile('/workspace/Workspace/UCF50.rar')\n",
    "\n",
    "    # Extract all files and directories from the 'UCF50.rar' archive to the DATA_PATH directory.\n",
    "    r.extractall(DATA_PATH)\n",
    "\n",
    "    # Close the RarFile object to release resources.\n",
    "    r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define constants\n",
    "SEQUENCE_LENGTH = 15  # Number of frames to use for each video\n",
    "IMAGE_SIZE = 224  # I3D expects 224x224 input\n",
    "SELECTED_CLASSES = ['Kayaking', 'Basketball', 'JumpRope']\n",
    "#SELECTED_CLASSES = ['Kayaking', 'Basketball', 'JumpRope', 'Diving', 'HorseRace', 'PullUps','MilitaryParade']\n",
    "DATASET_DIR = 'workspace/UCF50'\n",
    "BATCH_SIZE = 8  # Adjust based on your GPU memory\n",
    "NUM_EPOCHS = 5\n",
    "PPE_CLASSES = ['Hardhat', 'Mask', 'NO-Hardhat', 'NO-Mask', 'NO-Safety Vest', 'Person', 'Safety Cone', 'Safety Vest', 'machinery', 'vehicle']\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])  # Kinetics dataset mean and std\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract frames from a video\n",
    "def extract_frames(video_path):\n",
    "    frames = []\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    skip_frames_window = max(int(frame_count / SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    for _ in range(SEQUENCE_LENGTH):\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, _ * skip_frames_window)\n",
    "        success, frame = video_reader.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = transform(frame)\n",
    "        frames.append(frame)\n",
    "\n",
    "    video_reader.release()\n",
    "    \n",
    "    # If we don't have enough frames, we'll pad with zeros\n",
    "    while len(frames) < SEQUENCE_LENGTH:\n",
    "        frames.append(torch.zeros_like(frames[0]))\n",
    "    \n",
    "    return torch.stack(frames).permute(1, 0, 2, 3)  # [C, T, H, W]\n",
    "\n",
    "# Custom Dataset class\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = extract_frames(self.video_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        return frames, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load all video paths and labels\n",
    "def load_dataset():\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    for class_idx, class_name in enumerate(SELECTED_CLASSES):\n",
    "        class_dir = os.path.join(DATASET_DIR, class_name)\n",
    "        for video_name in os.listdir(class_dir):\n",
    "            video_path = os.path.join(class_dir, video_name)\n",
    "            video_paths.append(video_path)\n",
    "            labels.append(class_idx)\n",
    "    return video_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw graphs\n",
    "def draw_graphs(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss graph\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title('Total Loss vs Total Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy graph\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Model Total Accuracy vs Model Total Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_progress.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save preprocessed dataset\n",
    "def save_preprocessed_dataset(data, labels, file_path):\n",
    "    # Convert paths to strings if they're tensors\n",
    "    if isinstance(data[0], torch.Tensor):\n",
    "        data = [path.tolist() if isinstance(path, torch.Tensor) else path for path in data]\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump((data, labels), f)\n",
    "\n",
    "# Function to load preprocessed dataset\n",
    "def load_preprocessed_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    # Convert tensors to lists if necessary\n",
    "    if isinstance(data[0][0], torch.Tensor):\n",
    "        return ([path.tolist() if isinstance(path, torch.Tensor) else path for path in data[0]], data[1])\n",
    "    return data\n",
    "        \n",
    "# Function to preprocess the entire dataset\n",
    "def preprocess_dataset(video_paths, labels):\n",
    "    preprocessed_data = []\n",
    "    preprocessed_labels = []\n",
    "    for video_path, label in tqdm(zip(video_paths, labels), desc=\"Preprocessing dataset\", total=len(video_paths)):\n",
    "        frames = extract_frames(video_path)\n",
    "        preprocessed_data.append(frames)\n",
    "        preprocessed_labels.append(label)\n",
    "    return preprocessed_data, preprocessed_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and preprocessing dataset...\")\n",
    "\n",
    "#classes changed based on the dataset selected above.\n",
    "preprocessed_file = 'preprocessed_dataset_pytorch-3classes-mc3.pkl'\n",
    "#preprocessed_file = 'preprocessed_dataset_pytorch-7classes-mc3.pkl'\n",
    "\n",
    "if os.path.exists(preprocessed_file):\n",
    "    video_paths, labels = load_preprocessed_dataset(preprocessed_file)\n",
    "else:\n",
    "    video_paths, labels = load_dataset()\n",
    "    video_paths, labels = preprocess_dataset(video_paths, labels)\n",
    "    save_preprocessed_dataset(video_paths, labels, preprocessed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    video_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Create Dataset and DataLoader objects\n",
    "train_dataset = VideoDataset(train_paths, train_labels)\n",
    "test_dataset = VideoDataset(test_paths, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_counts = np.bincount(labels)\n",
    "class_weights = 1. / class_counts\n",
    "weights = torch.tensor([class_weights[label] for label in labels], dtype=torch.float)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained I3D model\n",
    "model = mc3_18(weights=MC3_18_Weights.KINETICS400_V1)\n",
    "\n",
    "# Modify the final classification layer for your number of classes\n",
    "num_classes = len(SELECTED_CLASSES)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for frames, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(frames)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(test_loader, desc=\"Validation\"):\n",
    "            frames, labels = frames.to(device), labels.to(device)\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Draw graphs after each epoch\n",
    "draw_graphs(train_losses, val_losses, train_accuracies, val_accuracies)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, evaluate the model on the test set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []  # For ROC and AUC\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for frames, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        frames, labels = frames.to(device), labels.to(device)\n",
    "        outputs = model(frames)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probabilities.extend(outputs.softmax(dim=1).cpu().numpy())  # For ROC and AUC\n",
    "\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "accuracy = 100 * sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"Overall Test Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot and save confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix (Percentage)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(all_labels, all_predictions, SELECTED_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "# Function to save classification report as a table with styling\n",
    "def save_classification_report(report_df, filename='classification_report.png'):\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))  # Increased size for better visibility\n",
    "    ax.axis('off')  # No axis for the table\n",
    "    \n",
    "    # Render the DataFrame as a table\n",
    "    table = ax.table(cellText=report_df.values,\n",
    "                     colLabels=report_df.columns,\n",
    "                     rowLabels=report_df.index,\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     bbox=[0, 0, 1, 1])  # Adjust bbox to fill the figure area\n",
    "    \n",
    "    # Customize table appearance\n",
    "    table.auto_set_font_size(False)  # Allow manual font size setting\n",
    "    table.set_fontsize(10)  # Set a readable font size\n",
    "    table.scale(1.2, 1.2)  # Scale table size\n",
    "\n",
    "    # Adjust column widths based on content\n",
    "    for i, width in enumerate([max(len(str(cell)) for cell in col) for col in report_df.T.values]):\n",
    "        table.auto_set_column_width([i])\n",
    "        table._cells[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        table._cells[(0, i)].set_facecolor('#4c72b0')  # Header color\n",
    "\n",
    "    for (i, j), cell in table._cells.items():\n",
    "        if i == 0:  # Header row\n",
    "            continue\n",
    "        cell.set_edgecolor('black')  # Cell border color\n",
    "        cell.set_facecolor('#f5f5f5')  # Alternate row color\n",
    "        cell.set_fontsize(10)\n",
    "        cell.set_text_props(color='black')\n",
    "\n",
    "    # Save as an image\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0.3)  # Increased padding for better fit\n",
    "    plt.close()\n",
    "\n",
    "# Classification Report generation\n",
    "report_dict = classification_report(all_labels, all_predictions, target_names=SELECTED_CLASSES, digits=4, output_dict=True)\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df['support'] = report_df['support'].astype(int)\n",
    "save_classification_report(report_df, 'classification_report.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot ROC and AUC for each class (if it's a multi-class problem)\n",
    "n_classes = len(SELECTED_CLASSES)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(all_labels) == i, np.array(all_probabilities)[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {SELECTED_CLASSES[i]} (AUC = {roc_auc[i]:0.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.savefig('roc_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'mc3_activity_recognition.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the ppe model. \n",
    "#source: https://www.kaggle.com/code/plasticglass/yolov8-safety-helmet-detection\n",
    "ppe_model = YOLO('ppe.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict on a single video with PPE detection.\n",
    "def predict_on_video(video_path):\n",
    "    model.eval()\n",
    "    frames = extract_frames(video_path).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    # Perform PPE detection on the last frame\n",
    "    last_frame = frames[0, :, -1].cpu().numpy()  # Shape: [C, H, W]\n",
    "    last_frame = np.transpose(last_frame, (1, 2, 0))  # Shape: [H, W, C]\n",
    "    last_frame = (last_frame * 255).astype(np.uint8)  # Convert to 0-255 range\n",
    "    last_frame_bgr = cv2.cvtColor(last_frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    ppe_results = ppe_model(last_frame_bgr)[0]\n",
    "    ppe_detections = [PPE_CLASSES[int(det.cls)] for det in ppe_results.boxes if int(det.cls) < len(PPE_CLASSES)]\n",
    "    \n",
    "    return SELECTED_CLASSES[predicted.item()], confidence.item(), ppe_detections\n",
    "# Modify the process_video function to include PPE detection\n",
    "def process_video(input_video_path, output_video_path):\n",
    "    video_reader = cv2.VideoCapture(input_video_path)\n",
    "    fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    frame_buffer = []\n",
    "    while True:\n",
    "        ret, frame = video_reader.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = transform(frame_rgb)\n",
    "        frame_buffer.append(frame_tensor)\n",
    "        \n",
    "        if len(frame_buffer) == SEQUENCE_LENGTH:\n",
    "            input_frames = torch.stack(frame_buffer).permute(1, 0, 2, 3).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_frames)\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                confidence, predicted = torch.max(probabilities, 1)\n",
    "            predicted_class = SELECTED_CLASSES[predicted.item()]\n",
    "            confidence_value = confidence.item()\n",
    "            \n",
    "            # Perform PPE detection\n",
    "            ppe_results = ppe_model(frame)[0]\n",
    "            ppe_detections = [PPE_CLASSES[int(det.cls)] for det in ppe_results.boxes if int(det.cls) < len(PPE_CLASSES)]\n",
    "            \n",
    "            # Display prediction, confidence, and PPE detections on frame\n",
    "            activity_text = f\"Activity: {predicted_class} ({confidence_value:.2f})\"\n",
    "            ppe_text = f\"PPE: {', '.join(ppe_detections) if ppe_detections else 'None'}\"\n",
    "            cv2.putText(frame, activity_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, ppe_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            \n",
    "            # Draw bounding boxes for PPE detections\n",
    "            for box in ppe_results.boxes:\n",
    "                class_id = int(box.cls)\n",
    "                if class_id < len(PPE_CLASSES):\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    cv2.putText(frame, PPE_CLASSES[class_id], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            \n",
    "            frame_buffer.pop(0)\n",
    "        \n",
    "        video_writer.write(frame)\n",
    "    \n",
    "    video_reader.release()\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predection function usage.\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directory where the video is stored\n",
    "    test_videos_directory = 'test_videos'\n",
    "\n",
    "    # Define the video title\n",
    "    video_title = 'mask'  # Replace with the actual video title\n",
    "\n",
    "    # Construct the input video file path\n",
    "    input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'\n",
    "\n",
    "    # Predict on a single video\n",
    "    prediction, confidence, ppe_detections = predict_on_video(input_video_file_path)\n",
    "    print(f\"Predicted activity: {prediction}, Confidence: {confidence:.2f}\")\n",
    "    print(f\"Detected PPE: {', '.join(ppe_detections) if ppe_detections else 'None'}\")\n",
    "\n",
    "    # Construct the output video path\n",
    "    output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'\n",
    "\n",
    "    # Process the video and save predictions\n",
    "    process_video(input_video_file_path, output_video_file_path)\n",
    "\n",
    "    # Display the output video\n",
    "    clip = VideoFileClip(output_video_file_path, audio=False, target_resolution=(300, None))\n",
    "    clip.ipython_display(fps=clip.fps if clip.fps else 24)  # Use the clip's fps or default to 24\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform frame-by-frame analysis.\n",
    "def frame_by_frame_analysis(video_path, model, transform, device, SELECTED_CLASSES, SEQUENCE_LENGTH):\n",
    "    model.eval()\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    frame_predictions = []\n",
    "    class_probabilities = defaultdict(list)\n",
    "    frame_buffer = []\n",
    "    \n",
    "    for _ in tqdm(range(frame_count), desc=\"Processing frames\"):\n",
    "        ret, frame = video_reader.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = transform(frame_rgb)\n",
    "        frame_buffer.append(frame_tensor)\n",
    "        \n",
    "        if len(frame_buffer) == SEQUENCE_LENGTH:\n",
    "            input_frames = torch.stack(frame_buffer).permute(1, 0, 2, 3).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_frames)\n",
    "                probabilities = F.softmax(outputs, dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            predicted_class = SELECTED_CLASSES[probabilities.argmax()]\n",
    "            frame_predictions.append(predicted_class)\n",
    "            \n",
    "            for i, class_name in enumerate(SELECTED_CLASSES):\n",
    "                class_probabilities[class_name].append(probabilities[i])\n",
    "            \n",
    "            frame_buffer.pop(0)\n",
    "        else:\n",
    "            frame_predictions.append(None)\n",
    "            for class_name in SELECTED_CLASSES:\n",
    "                class_probabilities[class_name].append(0)\n",
    "    \n",
    "    video_reader.release()\n",
    "    \n",
    "    # Pad the beginning of predictions and probabilities\n",
    "    pad_length = SEQUENCE_LENGTH - 1\n",
    "    frame_predictions = [None] * pad_length + frame_predictions[pad_length:]\n",
    "    for class_name in SELECTED_CLASSES:\n",
    "        class_probabilities[class_name] = [0] * pad_length + class_probabilities[class_name][pad_length:]\n",
    "    \n",
    "    return frame_predictions, class_probabilities, fps\n",
    "\n",
    "def plot_frame_by_frame_results(frame_predictions, class_probabilities, fps, output_path):\n",
    "    frame_count = len(frame_predictions)\n",
    "    time_axis = np.arange(frame_count) / fps\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot class probabilities\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for class_name, probs in class_probabilities.items():\n",
    "        plt.plot(time_axis, probs, label=class_name)\n",
    "    plt.title(\"Class Probabilities Over Time\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot predicted classes\n",
    "    plt.subplot(2, 1, 2)\n",
    "    unique_classes = list(set(frame_predictions) - {None})\n",
    "    class_to_num = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "    numeric_predictions = [class_to_num[cls] if cls is not None else -1 for cls in frame_predictions]\n",
    "    plt.scatter(time_axis, numeric_predictions, marker='.')\n",
    "    plt.yticks(range(len(unique_classes)), unique_classes)\n",
    "    plt.title(\"Predicted Class Over Time\")\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Predicted Class\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame-by-frame analysis function usage.\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directory where the video is stored\n",
    "    test_videos_directory = 'test_videos'\n",
    "\n",
    "    # Define the video title\n",
    "    video_title = 'mask'  # Replace with the actual video title\n",
    "\n",
    "    # Construct the input video file path\n",
    "    input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'\n",
    "\n",
    "    # Perform frame-by-frame analysis\n",
    "    frame_predictions, class_probabilities, fps = frame_by_frame_analysis(\n",
    "        input_video_file_path, model, transform, device, SELECTED_CLASSES, SEQUENCE_LENGTH\n",
    "    )\n",
    "\n",
    "    # Plot and save the results\n",
    "    output_graph_path = f'{test_videos_directory}/{video_title}_frame_analysis.png'\n",
    "    plot_frame_by_frame_results(frame_predictions, class_probabilities, fps, output_graph_path)\n",
    "\n",
    "    print(f\"Frame-by-frame analysis graph saved to: {output_graph_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Body straight detection for Pull Ups\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def predict_on_video(video_path):\n",
    "    model.eval()\n",
    "    frames = extract_frames(video_path).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    return SELECTED_CLASSES[predicted.item()], confidence.item()\n",
    "    \n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def check_body_straight(landmarks):\n",
    "    left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "    right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "    left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "    right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "    left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value]\n",
    "    right_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value]\n",
    "\n",
    "    # Check if body is straight (shoulders, hips, and ankles aligned)\n",
    "    body_straight = (\n",
    "        abs(left_shoulder.x - left_hip.x) < 0.1 and\n",
    "        abs(right_shoulder.x - right_hip.x) < 0.1 and\n",
    "        abs(left_hip.x - left_ankle.x) < 0.1 and\n",
    "        abs(right_hip.x - right_ankle.x) < 0.1\n",
    "    )\n",
    "\n",
    "    return body_straight\n",
    "\n",
    "def process_video(input_video_path, output_video_path):\n",
    "    video_reader = cv2.VideoCapture(input_video_path)\n",
    "    fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    frame_buffer = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while True:\n",
    "            ret, frame = video_reader.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_tensor = transform(frame_rgb)\n",
    "            frame_buffer.append(frame_tensor)\n",
    "            \n",
    "            # Perform pose estimation\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            \n",
    "            # Draw pose landmarks on the frame\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "            \n",
    "            if len(frame_buffer) == SEQUENCE_LENGTH:\n",
    "                input_frames = torch.stack(frame_buffer).permute(1, 0, 2, 3).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_frames)\n",
    "                    probabilities = F.softmax(outputs, dim=1)\n",
    "                    confidence, predicted = torch.max(probabilities, 1)\n",
    "                predicted_class = SELECTED_CLASSES[predicted.item()]\n",
    "                confidence_value = confidence.item()\n",
    "                \n",
    "                # Display activity and confidence\n",
    "                activity_text = f\"Activity: {predicted_class} ({confidence_value:.2f})\"\n",
    "                cv2.putText(frame, activity_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Check body straight for pull-ups\n",
    "                if pose_results.pose_landmarks:\n",
    "                    body_straight = check_body_straight(pose_results.pose_landmarks.landmark)\n",
    "                    posture_text = f\"Body straight: {'Yes' if body_straight else 'No'}\"\n",
    "                    cv2.putText(frame, posture_text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"No pose detected\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "                frame_buffer.pop(0)\n",
    "            \n",
    "            video_writer.write(frame)\n",
    "    \n",
    "    video_reader.release()\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Body straight detection function usage.\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directory where the video is stored\n",
    "    test_videos_directory = 'test_videos'\n",
    "\n",
    "    # Define the video title\n",
    "    video_title = 'fall2'  # Replace with the actual video title\n",
    "\n",
    "    # Construct the input video file path\n",
    "    input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'\n",
    "\n",
    "    # Predict on a single video\n",
    "    prediction, confidence = predict_on_video(input_video_file_path)\n",
    "    print(f\"Predicted activity: {prediction}, Confidence: {confidence:.2f}\")\n",
    "\n",
    "    # Construct the output video path\n",
    "    output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'\n",
    "\n",
    "    # Process the video and save predictions\n",
    "    process_video(input_video_file_path, output_video_file_path)\n",
    "\n",
    "    print(f\"Processed video saved as: {output_video_file_path}\")\n",
    "\n",
    "    # Display the output video\n",
    "    \n",
    "    # Get the duration of the video\n",
    "    cap = cv2.VideoCapture(output_video_file_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count / fps\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fall detection function.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def predict_on_video(video_path):\n",
    "    model.eval()\n",
    "    frames = extract_frames(video_path).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(frames)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    return SELECTED_CLASSES[predicted.item()], confidence.item()\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def detect_fall(landmarks):\n",
    "    # Get relevant landmark positions\n",
    "    nose = landmarks[mp_pose.PoseLandmark.NOSE.value]\n",
    "    left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
    "    right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
    "    left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value]\n",
    "    right_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value]\n",
    "\n",
    "    # Calculate the average y-position of hips and ankles\n",
    "    hip_y = (left_hip.y + right_hip.y) / 2\n",
    "    ankle_y = (left_ankle.y + right_ankle.y) / 2\n",
    "\n",
    "    # Check if the nose is below the hips or if the body is horizontal\n",
    "    fall_detected = nose.y > hip_y or abs(hip_y - ankle_y) < 0.1\n",
    "\n",
    "    return fall_detected\n",
    "\n",
    "def process_video(input_video_path, output_video_path):\n",
    "    video_reader = cv2.VideoCapture(input_video_path)\n",
    "    fps = int(video_reader.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    frame_buffer = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while True:\n",
    "            ret, frame = video_reader.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_tensor = transform(frame_rgb)\n",
    "            frame_buffer.append(frame_tensor)\n",
    "            \n",
    "            # Perform pose estimation\n",
    "            pose_results = pose.process(frame_rgb)\n",
    "            \n",
    "            # Draw pose landmarks on the frame\n",
    "            if pose_results.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "            \n",
    "            if len(frame_buffer) == SEQUENCE_LENGTH:\n",
    "                input_frames = torch.stack(frame_buffer).permute(1, 0, 2, 3).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_frames)\n",
    "                    probabilities = F.softmax(outputs, dim=1)\n",
    "                    confidence, predicted = torch.max(probabilities, 1)\n",
    "                predicted_class = SELECTED_CLASSES[predicted.item()]\n",
    "                confidence_value = confidence.item()\n",
    "                \n",
    "                # Display activity and confidence\n",
    "                activity_text = f\"Activity: {predicted_class} ({confidence_value:.2f})\"\n",
    "                cv2.putText(frame, activity_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                # Check for fall\n",
    "                if pose_results.pose_landmarks:\n",
    "                    fall_detected = detect_fall(pose_results.pose_landmarks.landmark)\n",
    "                    fall_text = f\"Fall detected: {'Yes' if fall_detected else 'No'}\"\n",
    "                    cv2.putText(frame, fall_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"No pose detected\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "                frame_buffer.pop(0)\n",
    "            \n",
    "            video_writer.write(frame)\n",
    "    \n",
    "    video_reader.release()\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define the directory where the video is stored\n",
    "    test_videos_directory = 'test_videos'\n",
    "\n",
    "    # Define the video title\n",
    "    video_title = 'fall2'  # Replace with the actual video title\n",
    "\n",
    "    # Construct the input video file path\n",
    "    input_video_file_path = f'{test_videos_directory}/{video_title}.mp4'\n",
    "\n",
    "    # Predict on a single video\n",
    "    prediction, confidence = predict_on_video(input_video_file_path)\n",
    "    print(f\"Predicted activity: {prediction}, Confidence: {confidence:.2f}\")\n",
    "\n",
    "    # Construct the output video path\n",
    "    output_video_file_path = f'{test_videos_directory}/{video_title}-Output-SeqLen{SEQUENCE_LENGTH}.mp4'\n",
    "\n",
    "    # Process the video and save predictions\n",
    "    process_video(input_video_file_path, output_video_file_path)\n",
    "\n",
    "    print(f\"Processed video saved as: {output_video_file_path}\")\n",
    "\n",
    "    # Display the output video\n",
    "    \n",
    "    # Get the duration of the video\n",
    "    cap = cv2.VideoCapture(output_video_file_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = frame_count / fps\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRA:\n",
    "# Function to download videos from YouTube.\n",
    "def download_yt_videos(yt_url_list, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'outtmpl': os.path.join(save_dir, '%(title)s.%(ext)s'),\n",
    "        'format': 'bestvideo+bestaudio/best',\n",
    "        'merge_output_format': 'mp4'\n",
    "    }\n",
    "    \n",
    "    for url in yt_url_list:\n",
    "        try:\n",
    "            with YoutubeDL(ydl_opts) as ydl:\n",
    "                ydl.download([url])\n",
    "                print(f\"Downloaded: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "yt_url_list = [\n",
    "    'https://www.youtube.com/shorts/PbA0JXVph8E'\n",
    "    \n",
    "]\n",
    "save_dir = 'test_data'\n",
    "download_yt_videos(yt_url_list, save_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
